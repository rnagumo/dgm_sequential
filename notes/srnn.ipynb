{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "pytorch",
   "display_name": "pytorch"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pixyz.distributions as pxd\n",
    "import pixyz.losses as pxl\n",
    "import pixyz.models as pxm"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardRNN(pxd.Deterministic):\n",
    "    def __init__(self, u_dim, d_dim):\n",
    "        super().__init__(cond_var=[\"u\", \"d_prev\"], var=[\"d\"])\n",
    "\n",
    "        self.rnn_cell = nn.GRUCell(u_dim, d_dim)\n",
    "        self.d0 = nn.Parameter(torch.zeros(1, 1, d_dim))\n",
    "\n",
    "    def forward(self, u, d_prev):\n",
    "        d = self.rnn_cell(u, d_prev)\n",
    "        return {\"d\": d}\n",
    "\n",
    "class Prior(pxd.Normal):\n",
    "    def __init__(self, d_dim, z_dim):\n",
    "        super().__init__(cond_var=[\"z_prev\", \"d\"], var=[\"z\"])\n",
    "\n",
    "        self.fc1 = nn.Linear(d_dim + z_dim, 512)\n",
    "        self.fc21 = nn.Linear(512, z_dim)\n",
    "        self.fc22 = nn.Linear(512, z_dim)\n",
    "\n",
    "    def forward(self, z_prev, d):\n",
    "        h = F.relu(self.fc1(torch.cat([z_prev, d], dim=-1)))\n",
    "        scale = self.fc21(h)\n",
    "        loc = F.softplus(self.fc22(h))\n",
    "        return {\"scale\": scale, \"loc\": loc}\n",
    "\n",
    "class Generator(pxd.Bernoulli):\n",
    "    def __init__(self, z_dim, d_dim, x_dim):\n",
    "        super().__init__(cond_var=[\"z\", \"d\"], var=[\"x\"])\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim + d_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, x_dim)\n",
    "\n",
    "    def forward(self, z, d):\n",
    "        h = F.relu(self.fc1(torch.cat([z, d], dim=-1)))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        probs = torch.sigmoid(self.fc3(h))\n",
    "        return {\"probs\": probs}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardRNN(pxd.Deterministic):\n",
    "    def __init__(self, x_dim, d_dim, a_dim):\n",
    "        super().__init__(cond_var=[\"x\", \"d\"], var=[\"a\"])\n",
    "\n",
    "        self.rnn = nn.GRU(x_dim + d_dim, a_dim, bidirectional=True)\n",
    "        self.a0 = nn.Parameter(torch.zeros(2, 1, a_dim))\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        a0 = self.a0.expand(2, x.size(1), self.a0.size(2)).contiguous()\n",
    "        a, _ = self.rnn(torch.cat([x, d], dim=-1), a0)\n",
    "        return {\"a\": a[:, :, a.size(2) // 2:]}\n",
    "\n",
    "\n",
    "class VariationalPrior(pxd.Normal):\n",
    "    def __init__(self, z_dim, a_dim):\n",
    "        super().__init__(cond_var=[\"z_prev\", \"a\"], var=[\"z\"])\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim + a_dim, 512)\n",
    "        self.fc21 = nn.Linear(512, z_dim)\n",
    "        self.fc22 = nn.Linear(512, z_dim)\n",
    "\n",
    "    def forward(self, z_prev, a):\n",
    "        h = F.relu(self.fc1(torch.cat([z_prev, a], dim=-1)))\n",
    "        scale = self.fc21(h)\n",
    "        loc = F.softplus(self.fc22(h))\n",
    "        return {\"scale\": scale, \"loc\": loc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 2\n",
    "t_dim = 5\n",
    "device = \"cpu\"\n",
    "u_dim = x_dim\n",
    "\n",
    "# Latent dimension\n",
    "d_dim = 3\n",
    "z_dim = 4\n",
    "a_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Prior(d_dim, z_dim).to(device)\n",
    "frnn = ForwardRNN(u_dim, d_dim).to(device)\n",
    "decoder = Generator(z_dim, d_dim, x_dim).to(device)\n",
    "brnn = BackwardRNN(x_dim, d_dim, a_dim).to(device)\n",
    "encoder = VariationalPrior(z_dim, a_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Distribution:\n  p(z|z_{prev},d)\nNetwork architecture:\n  Prior(\n    name=p, distribution_name=Normal,\n    var=['z'], cond_var=['z_prev', 'd'], input_var=['z_prev', 'd'], features_shape=torch.Size([])\n    (fc1): Linear(in_features=7, out_features=512, bias=True)\n    (fc21): Linear(in_features=512, out_features=4, bias=True)\n    (fc22): Linear(in_features=512, out_features=4, bias=True)\n  )\nDistribution:\n  p(d|u,d_{prev})\nNetwork architecture:\n  ForwardRNN(\n    name=p, distribution_name=Deterministic,\n    var=['d'], cond_var=['u', 'd_prev'], input_var=['u', 'd_prev'], features_shape=torch.Size([])\n    (rnn_cell): GRUCell(2, 3)\n  )\nDistribution:\n  p(x|z,d)\nNetwork architecture:\n  Generator(\n    name=p, distribution_name=Bernoulli,\n    var=['x'], cond_var=['z', 'd'], input_var=['z', 'd'], features_shape=torch.Size([])\n    (fc1): Linear(in_features=7, out_features=512, bias=True)\n    (fc2): Linear(in_features=512, out_features=256, bias=True)\n    (fc3): Linear(in_features=256, out_features=2, bias=True)\n  )\nDistribution:\n  p(a|x,d)\nNetwork architecture:\n  BackwardRNN(\n    name=p, distribution_name=Deterministic,\n    var=['a'], cond_var=['x', 'd'], input_var=['x', 'd'], features_shape=torch.Size([])\n    (rnn): GRU(5, 2, bidirectional=True)\n  )\nDistribution:\n  p(z|z_{prev},a)\nNetwork architecture:\n  VariationalPrior(\n    name=p, distribution_name=Normal,\n    var=['z'], cond_var=['z_prev', 'a'], input_var=['z_prev', 'a'], features_shape=torch.Size([])\n    (fc1): Linear(in_features=6, out_features=512, bias=True)\n    (fc21): Linear(in_features=512, out_features=4, bias=True)\n    (fc22): Linear(in_features=512, out_features=4, bias=True)\n  )\n"
    }
   ],
   "source": [
    "print(prior)\n",
    "print(frnn)\n",
    "print(decoder)\n",
    "print(brnn)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "- \\mathbb{E}_{p(z,d|z_{prev},a,u,d_{prev})} \\left[\\log p(x|z,d) \\right]\n"
    }
   ],
   "source": [
    "ce = pxl.CrossEntropy(encoder * frnn, decoder)\n",
    "print(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "D_{KL} \\left[p(z|z_{prev},a)||p(z|z_{prev},d) \\right]\n"
    }
   ],
   "source": [
    "kl = pxl.KullbackLeibler(encoder, prior)\n",
    "print(kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "minibatch_size = 1\n",
    "\n",
    "data = {\n",
    "    \"z_prev\": torch.zeros(minibatch_size, z_dim).to(device),\n",
    "    \"d_prev\": torch.zeros(minibatch_size, d_dim).to(device),\n",
    "    \"u\": torch.zeros(minibatch_size, x_dim).to(device),\n",
    "    \"dummy\": torch.zeros(minibatch_size, x_dim).to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (prior * frnn * decoder).sample(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'z_prev': tensor([[0., 0., 0., 0.]]),\n 'd_prev': tensor([[0., 0., 0.]]),\n 'u': tensor([[0., 0.]]),\n 'dummy': tensor([[0., 0.]]),\n 'd': tensor([[-0.0011,  0.0067,  0.1337]], grad_fn=<AddBackward0>),\n 'z': tensor([[0.7717, 0.7760, 0.6791, 0.6562]]),\n 'x': tensor([[0., 0.]])}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['z_prev', 'u', 'd_prev']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prior * frnn * decoder).input_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.5095, 0.4868]], grad_fn=<SigmoidBackward>)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = decoder.sample_mean({\"z\": sample[\"z\"], \"d\": sample[\"d\"]})\n",
    "\n",
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}